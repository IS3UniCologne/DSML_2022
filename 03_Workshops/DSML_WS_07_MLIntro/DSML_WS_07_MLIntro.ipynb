{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `DSML Workshop 07` - Introduction to ML\n",
    "\n",
    "In this workshop we start with hands-on machine learning focusing mostly on the linear regression example covererd previously in the lecture.\n",
    "\n",
    "Machine learning is a fast-growing field. Recent popularity of machine learning as a topic can largely be explained due to a combination of three different elements: \n",
    "1. Gains in computing power (machine learning, as it grew out of computer science, has always been fundamentally concerned with computational algorithms)\n",
    "1. Massive amounts of available data (the \"raw materials\" for machine learning methods)\n",
    "1. Some notable algorithmic advances that have occurred over the last 30 years.  \n",
    "\n",
    "To start, though, we will cover some of the most basic algorithms to codify the underlying principles.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: predicting peak electrical power\n",
    "\n",
    "Let's consider the above problem a bit more formally, starting with a simple\n",
    "example.  Suppose you want to predict what the peak electricity demand will be\n",
    "during the day tomorrow for some area (we'll consider data from the area\n",
    "surrounding Pittsburgh, PA).  This is actually a very important problem from a\n",
    "logitics planning perspective: electricity generators, which for the most part\n",
    "are based upon boiling water to move turbines, cannot turn on instantly, so in\n",
    "order to guarantee that we have enough power to supply a given area, a system\n",
    "operator typically needs to have some excess generation always waiting in the\n",
    "wings.  The better we can forecast future demand, the smaller our excess margin\n",
    "can be, leading to increased efficiency of the entire electrical grid.  \n",
    "The power consumption tomorrow depends on many factors: temperature, day of\n",
    "week, season, holiday events, etc, not to mention some inherrent randomness\n",
    "that we don't expect to even predict with perfect accuracy.  However, even for\n",
    "someone working in the area, it would be very\n",
    "difficult to come up with a model for electrical demand based soley upon \"first\n",
    "principles\", thinking about the nature of electricity consumption or the devices\n",
    "people may use, in an attempt to predict future consumption.\n",
    "\n",
    "What _is_ easy, however, is simply to collect lots of data about past energy\n",
    "consumption (the system operator serving the Pittsburgh region, PJM, maintains\n",
    "a data set available [here](https://dataminer2.pjm.com/feed/hrl_load_metered/definition)) \n",
    "as well as the past factors that affect consumption, like the past weather for\n",
    "the area. The files we are loading are the raw files we downloaded from this source. The final input data for our code is `Pittsburgh_load_data.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "register_matplotlib_converters()\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise** Import the data and have a first look.\n",
    "\n",
    "If you have time left, also convert the 'Date' column to datetime.\n",
    "Hint: the pandas function `to_datetime()` might be useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your Code goes here\n",
    "df = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today we want to build a simple ML model to forecast peak demand.\n",
    "Using the converted Date column, we can have a look at the interaction between the date and the peak demand, which can be found under the 'MAX' column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a plot, plotting the date on the x-axis and the peak demand on the y-axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Target solution 5 min\n",
    "fig, ax = plt.subplots(figsize = (16,9))\n",
    "\n",
    "# Plotting the data\n",
    "ax.plot(df[\"Date\"],df[[\"MAX\"]])\n",
    "ax.set_xlabel(\"Date\")\n",
    "ax.set_ylabel(\"Peak Demand (GW)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: In the next steps we want to look at the summer months only. Create a `df_summer` which only contains the months of June, July and August. Hint: use the `dt.month` function to extract the month from the datetime object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a dataframe df_summer which only contains the months of June, July and August\n",
    "df_summer = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let us define our dependent (y) and independent (x) variables for peak electricity load prediction\n",
    "\n",
    "xp = df_summer['High_temp']\n",
    "\n",
    "yp = df_summer['MAX']\n",
    "\n",
    "#min_H_temp, max_H_temp = xp.min(), xp.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (8,6))\n",
    "# Plotting the data\n",
    "ax.scatter(xp, yp, marker='x',)\n",
    "ax.set_xlabel(\"High Temperature (°C)\")\n",
    "ax.set_ylabel(\"Peak Demand (GW)\")\n",
    "\n",
    "plt.show()\n",
    "#saving figures (You can comment-out this line inorder not to save figures)\n",
    "#plt.savefig('summer data_peak demand.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it turns out, electricity consumption in the summer in Pittsburgh is largely\n",
    "driven by air conditioning, so with increasing high temperature comes increasing\n",
    "electrical demand.  Thus, we may hypothesize that we can form a fairly good prediction of the peak demand using a linear model: that is, we hypothesize that \n",
    "\\begin{equation}\n",
    "\\mathrm{PeakDemand} \\approx \\theta_1 \\cdot \\mathrm{HighTemperature} + \\theta_2\n",
    "\\end{equation}\n",
    "where $\\theta_1$ is the slope of the line and $\\theta_2$ is the intercept term (together called the _parameters_ of the model).  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define theta vector\n",
    "theta = np.array([0, 2.0])\n",
    "\n",
    "# set up plot\n",
    "fig, ax = plt.subplots(figsize = (8,6))\n",
    "\n",
    "# add objects to axis\n",
    "ax.scatter(xp, yp, marker='x')\n",
    "xlimp, ylimp =(plt.gca().get_xlim(), plt.gca().get_ylim())  #gets limity of x and y\n",
    "ax.plot(xlimp, [theta[0]*xlimp[0]+theta[1], theta[0]*xlimp[1]+theta[1]], 'C1')\n",
    "ax.set_xlim(xlimp)\n",
    "ax.set_ylim(ylimp)\n",
    "ax.set_xlabel(\"High Temperature (°C)\")\n",
    "ax.set_ylabel(\"Peak Demand (GW)\")\n",
    "plt.show()\n",
    "#plt.savefig('summer data_peak demand_line.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naturally, this model won't fit the data exactly (we can see from the chart that the figure doesn't lie precisely on an exact line), but if we can find slope and intercept terms that fit the data well, then for example, if we want to know what the peak demand will be tomorrow we can simply plug in the forecasted high temperature into the equation above get an estimate of the peak demand tomorrow (ignore the fact, for now that the high temperature tomorrow is also a prediction, we'll assume we just get this from a reliable source, and domains like weather forecasting are extremely well-studied in practice). This is of course equivalent to just \"finding a point on the line\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding good parameters\n",
    "\n",
    "The question, of course, is how we find \"good\" values for $\\theta_1$ and $\\theta_2$ that fit this data well, i.e., so that the line fits the data as \"closely\" as possible.  The method we will describe for doing this (which is called _gradient descent_) is probably not the simplest algorithm for finding this fit.  In fact, as we will see, there is a very simple closed-form expression that will immediately give us the same solution for the framework we consider here.  But gradient descent is an _extremely_ powerful and general algorithm (and _is_ actually quite simple compared to some alternative approaches), and it is no exaggeration to say that gradient descent underlies virtually all modern machine learning.  So, with these caveats in place, let's dive in to understanding how we find \"good\" parameters $\\theta_1$ and $\\theta_2$ in some disciplined manner.\n",
    "\n",
    "**Objective functions**  In order to find good values for the parameters, we need to formally define what \"good\" means in this setting.  This will actually be one of the key questions for machine learning algorithms in general, and difference notions of goodness lead to different algorithms.  Fortunately, there are some very well-studied definitions in this context, and so we have some \"standard\" options that we can try.  The notion that we will consider here captures the idea of the \"squared error\" between the prediction and the actual values.  That is, we consider all the days in the plot above, where $\\mathrm{HighTemperature}^{(i)}$ denotes the high temperature and $\\mathrm{PeakDemand}^{(i)}$ denotes the peak demand on day $i$.  Since _predicted_ peak demand for day $i$ is equal to\n",
    "\\begin{equation}\n",
    "\\theta_1 \\cdot \\mathrm{HighTemperature} + \\theta_2\n",
    "\\end{equation}\n",
    "we want to make this quantity as close as possible, averaged over all the days, to the true $\\mathrm{PeakDemand}^{(i)}$.  We're going to measure this closeness in terms of the squared difference between the predicted and actual more.  More formally, we would like to minimize the quantity:\n",
    "\\begin{equation}\n",
    "\\frac{1}{\\# \\mathrm{days}} \\sum_{i \\in \\mathrm{days}} \\left ( \\theta_1 \\cdot \\mathrm{HighTemperature}^{(i)} + \\theta_2 - \\mathrm{PeakDemand}^{(i)} \\right )^2 \\equiv E(\\theta)\n",
    "\\end{equation}\n",
    "which we abbreviate as $E(\\theta)$ to emphasize the fact that we are going to be minimizing this error by tuning our $\\theta$ variables.  This is known as the _objective function_ that we are trying to minimize.  A natural question that you may be asking is: why did we choose to measure closeness using this squared difference?  Why not use the average of absolute difference?  Or the maximum absolute difference?  These are good questions, and we'll defer answering them for now, except to say that we will definitely consider other possibilities later.  The squared error is simply a very common choice, mainly for reasons of mathematical convenience.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data scaling**  We'll shortly see what the gradient descent procedure looks like in our example above.  Before we apply the algorithm, though, we're going to make one small modification to our problem, and _normalize_ the data (both the inputs and the output) before we attempt to run the gradient descent algorithm.  We will see the reason for this more clearly shortly, but the brief reason is that if our slope and intercept terms are on very different \"scales\" (which would be the case here, because of the relatively large input values (units of degrees Celsius) compared to the output values (units of gigawatts)), then we would actually need to take very different step sizes in the two parameters $\\theta_1$ and $\\theta_2$.  This is possible to manually tune in our case, but when we start having many more parameters, it's not feasible.  We thus make our life much easier if we scale all the input and output data to be in the same rough range _before_ running gradient descent (Note: in class we talked about only scaling the input, which also happens to work fine here, but I think it may be easier conceptually to consider the case where we just normalize all the inputs and outputs in the same manner).\n",
    "\n",
    "We can re-scale data in a number of ways, but a simple strategy is just to translate and scale the coordinates such that the values vary between zero and one in our dataset.  This can be easily achieved by the transformation\n",
    "\\begin{equation}\n",
    "\\tilde{x}^{(i)} = \\frac{x^{(i)} - \\min_i x^{(i)}}{\\max_i x^{(i)} - \\min_i x^{(i)}}\n",
    "\\end{equation}\n",
    "and similarly for $\\tilde{y}^{(i)}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Re-scale input and output features using the formula shown above and plot the data. Call the normalized data vectors `x_nor` and `y_nor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "x_nor = None\n",
    "y_nor = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up plot\n",
    "fig, axes = plt.subplots(1,2,figsize = (16,9))\n",
    "\n",
    "axes[0].scatter(xp, yp, marker='x')\n",
    "axes[0].set_xlabel(\"Re-scaled Temperature\")\n",
    "axes[0].set_ylabel(\"Re-scaled Demand\")\n",
    "\n",
    "axes[1].scatter(x_nor, y_nor, marker = 'x')\n",
    "axes[1].set_xlabel(\"Re-scaled Temperature\")\n",
    "axes[1].set_ylabel(\"Re-scaled Demand\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This look identical to the previous plot, of course, except that the units no longer correspond to traditional quantities like degrees Celsius or gigawatts, but just some linear transformation of these units."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing gradient descent\n",
    "\n",
    "Now let's look at the gradient descent algorithm, which we have derived mathematically in previous lectures.  This will initialize $\\theta_1$ and $\\theta_2$ to zero and repeatedly update them according to the partial derivative rules.  We will use the step size (also known as learning rate) $\\alpha=1$, and print out the value of $\\theta$ per each iteration.\n",
    "\n",
    "\\begin{equation}\n",
    "\\theta_j  := \\theta_j  − \\alpha \\sum_{j=1}^m (\\sum_{i=1}^n \\theta_j x_j^{(i)} - y^{(i)})x_j^{(i)}\n",
    "\\end{equation}\n",
    "\n",
    "Note: The above simplified equation omits the number of days (n) and the *2 term which results from partial differentiation and groups them into the alpha term for easier representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = np.array([0., 0.])\n",
    "\n",
    "alpha = 1.0\n",
    "\n",
    "for t in range(20):\n",
    "    print(\"Iteration {}: \".format(t), theta)\n",
    "    \n",
    "    # partial derivative theta1: 2*sum(((theta1*x + theta2)-y)*x)\n",
    "    # term 2/N added here (usully included in alpha term), this is just a constant so could also be omitted\n",
    "    theta[0] -= alpha/len(xp) * 2 * sum((theta[0] * x_nor + theta[1] - y_nor)*x_nor)\n",
    "    \n",
    "    # partial derivative theta2: 2*sum(((theta1*x + theta2)-y))\n",
    "    # term 2/N added here (usully included in alpha term), this is just a constant so could also be omitted\n",
    "    theta[1] -= alpha/len(xp) * 2 * sum((theta[0] * x_nor + theta[1] - y_nor) ) # this is the intercept with a slightly different partial derivative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize what this looks like in a couple different ways.  First, let's look at what our line looks like during different iterations of gradient descent.  For this purpose, we'll wrap the above in a simple function that takes `iters` iterations of gradient descent (note that we can of course get all these plots within a single run of gradient descent, but we'll just use multiple calls to this function for illustration purposes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(iters):\n",
    "    theta = np.array([0., 0.])\n",
    "    alpha = 1.0\n",
    "    for t in range(iters):\n",
    "        theta[0] -= alpha/len(xp) * 2 * sum((theta[0] * x_nor + theta[1] - y_nor)*x_nor)\n",
    "        theta[1] -= alpha/len(xp) * 2 * sum((theta[0] * x_nor + theta[1] - y_nor) )\n",
    "    return theta\n",
    "\n",
    "def plot_fit(theta):\n",
    "    \n",
    "    Error = sum((theta[0]*x_nor + theta[1] - y_nor)**2) # simple least squares error, which underlies our OLS example\n",
    "    \n",
    "    # compute partial derivative (i.e. gradient) for theta 1 and 2\n",
    "    def_theta1 = sum((theta[0] * x_nor + theta[1] - y_nor)*x_nor)\n",
    "    def_theta2 = sum((theta[0] * x_nor + theta[1] - y_nor))\n",
    "    \n",
    "    # plot\n",
    "    \n",
    "    plt.figure(figsize = (8,6))\n",
    "    plt.scatter(x_nor, y_nor, marker = 'x')\n",
    "    plt.xlabel(\"Nomalized Temperature\")\n",
    "    plt.ylabel(\"Normalized Demand\")\n",
    "    xlim, ylim =(plt.gca().get_xlim(), plt.gca().get_ylim())\n",
    "    \n",
    "    plt.plot(xlim, [theta[0]*xlim[0]+theta[1], theta[0]*xlim[1]+theta[1]], 'C1')\n",
    "    plt.xlim(xlim)\n",
    "    plt.ylim(ylim)\n",
    "    print('Theta = ', theta, 'Error = ',Error,'def_theta1 = ',def_theta1, 'def_theta2 = ', def_theta2 )\n",
    "    #plt.savefig('gradient decent '+str(theta[0])+'.png', dpi = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_fit(gradient_descent(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_fit(gradient_descent(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_fit(gradient_descent(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_fit(gradient_descent(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_fit(gradient_descent(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_fit(gradient_descent(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_fit(gradient_descent(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_fit(gradient_descent(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_fit(gradient_descent(1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error versus iteration\n",
    "\n",
    "We can also look at the average error versus iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_err(iters):\n",
    "    err = []\n",
    "    theta = np.array([0., 0.])\n",
    "    alpha = 1.0\n",
    "    for t in range(iters):\n",
    "        err.append(np.mean((theta[0] * x_nor + theta[1] - y_nor)**2))\n",
    "        theta[0] -= alpha/len(xp) * 2 * sum((theta[0] * x_nor + theta[1] - y_nor)*x_nor)\n",
    "        theta[1] -= alpha/len(xp) * 2 * sum((theta[0] * x_nor + theta[1] - y_nor) )\n",
    "    return np.array(err)\n",
    "plt.figure(figsize = (8,6))\n",
    "plt.plot(np.arange(0,50), gradient_descent_err(50))\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Average error\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the answer back in the original coordinates\n",
    "\n",
    "Fortunately, we don't need to resort to solving the system in the original coordinates, we can simply solve on our normalized data and then find the corresponding equations for the original data.  Specifically, since our model gives the approximation\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\tilde{y} & \\approx \\tilde{x} \\cdot \\theta_1 + \\theta_2\\\\\n",
    "\\Longrightarrow \\;\\; \\frac{y-a}{b} & \\approx \\frac{x-c}{d} \\cdot \\theta_1 + \\theta_2 \\\\\n",
    "\\Longrightarrow \\;\\; y-a & \\approx ((x-c)\\cdot \\theta_1)/d)\\cdot b + b \\theta_2 \\\\\n",
    "\\Longrightarrow \\;\\; y & \\approx x \\cdot (b \\theta_1/d) + b \\theta_2 + a - c b \\theta_1/d\\\\\n",
    "\\Longrightarrow \\;\\; y & \\approx x \\cdot \\hat{\\theta}_1 + \\hat{\\theta}_2\\\\\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "where\n",
    "\\begin{equation}\n",
    "a = \\min_i y^{(i)}, \\;\\; b = \\max_i y^{(i)} - \\min_i y^{(i)}, \\;\\; c = \\min_i x^{(i)}, \\;\\; d = \\max_i x^{(i)} - \\min_i x^{(i)}, \n",
    "\\end{equation}\n",
    "and where we define\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\hat{\\theta}_1 & = b \\theta_1/d \\\\\n",
    "\\hat{\\theta}_2 & = b \\theta_2 + a - c \\cdot(b \\theta_1/d).\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "That might seem like a lot, but all it's saying is that there is an easy formula to convert between the solution we get for the normalized data and the unnormalized data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the answer back in the original coordinates\n",
    "\n",
    "Fortunately, we don't need to resort to solving the system in the original coordinates, we can simply solve on our normalized data and then find the corresponding equations for the original data.  Specifically, since our model gives the approximation\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\tilde{y} & \\approx \\tilde{x} \\cdot \\theta_1 + \\theta_2\\\\\n",
    "\\Longrightarrow \\;\\; \\frac{y-a}{b} & \\approx \\frac{x-c}{d} \\cdot \\theta_1 + \\theta_2 \\\\\n",
    "\\Longrightarrow \\;\\; y-a & \\approx ((x-c)\\cdot \\theta_1)/d)\\cdot b + b \\theta_2 \\\\\n",
    "\\Longrightarrow \\;\\; y & \\approx x \\cdot (b \\theta_1/d) + b \\theta_2 + a - c b \\theta_1/d\\\\\n",
    "\\Longrightarrow \\;\\; y & \\approx x \\cdot \\hat{\\theta}_1 + \\hat{\\theta}_2\\\\\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "where\n",
    "\\begin{equation}\n",
    "a = \\min_i y^{(i)}, \\;\\; b = \\max_i y^{(i)} - \\min_i y^{(i)}, \\;\\; c = \\min_i x^{(i)}, \\;\\; d = \\max_i x^{(i)} - \\min_i x^{(i)}, \n",
    "\\end{equation}\n",
    "and where we define\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\hat{\\theta}_1 & = b \\theta_1/d \\\\\n",
    "\\hat{\\theta}_2 & = b \\theta_2 + a - c \\cdot(b \\theta_1/d).\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "That might seem like a lot, but all it's saying is that there is an easy formula to convert between the solution we get for the normalized data and the unnormalized data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = gradient_descent(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Using the above expression, re-scale the theta parameters to the original scale by creating a new array entitled `theta_hat`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_hat  # not that "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we plot the derived linear regression line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize = (8,6))\n",
    "\n",
    "\n",
    "ax.scatter(xp, yp, marker='x')\n",
    "\n",
    "xlim, ylim =(plt.gca().get_xlim(), plt.gca().get_ylim())\n",
    "ax.plot(xlim, [theta_hat[0]*xlim[0]+theta_hat[1], theta_hat[0]*xlim[1]+theta_hat[1]], 'C1')\n",
    "\n",
    "ax.set_xlim(xlim)\n",
    "ax.set_ylim(ylim)\n",
    "ax.set_xlabel(\"High Temperature (°C)\")\n",
    "ax.set_ylabel(\"Peak Demand (GW)\")\n",
    "print(theta, theta_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Libraries for machine learning in Python`\n",
    "\n",
    "Finally, we conclude with some information about the types of libraries we will use to run machine learning algorithms in Python.  Although there are a number of machine learning packages available, by far the most popular Python library for general-purpose \"classical\" machine learning (this is in contrast to packages focused specficially on deep learning, such as [TensorFlow](http://www.tensorflow.org) and [Keras](https://keras.io/getting_started/)) is the [scikit-learn](http://scikit-learn.org/) library.  Scikit Learn is general purpose machine learning library with a number of common machine learning algorithms built in.\n",
    "\n",
    "One important note, however, is that (despite some ongoing efforts to make it more scalable), scikit-learn is still best suited for small to medium-scale problems (say with ~10,000s of examples and ~1,000s of features).  For these size problems, most of the algorithms contained in the library will work reasonably fast, and the library has the advantage that one can train many different types of algorithms all with the same interface.  However, if you have data that is much bigger than this, then the algorithms start to get fairly slow compared to other more specialized libraries, and you are likely better off using an alternative library.\n",
    "\n",
    "Another important caveat, and this is one that sadly often gets ignored, is that unlikely other software libraries, you _need_ to have some (even just basic) understanding of what the algorithms do in order to use scikit-learn effectively.  This is because virtually all algorithms will have some substantial number of hyperparameters, settings to the algorithm that can drastically effect performance (and really, affect _all_ the underlying aspects of the algorithm itself, the hypothesis, loss, and optimization problem).  Sadly, a surprisingly large number of the statements people make about data science techniques seem less about the actual algorithms and more about whatever default settings scikit-learn happens to have for each algorithm.  This is why you get people saying things like \"support vector machines worked better than neural networks for this problem\", which is a completely meaningless statement unless you know _what sort_ of support vector machine, and _what architecture_ neural network.  Maybe in 10 years we will be at a place where the ML algorithms truly are \"self-contained\", and practitioners don't need to know anything about the underlying algorithms to get good performance (certainly, some researchers and companies are attempting to develop tools that move in this direction).  But for the vast majority of tasks, we are still a lot ways away from this point, and you do absolutely need to understand the algorithms to get reasonable performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear regression in scikit-learn\n",
    "\n",
    "Let's look at how we specify a model, fit it to data, and make predictions in scikit-learn.  These three tasks form the common usage pattern for most interation with scikit-learn.  Let's first prepare our data.  Note that scikit-learn by default will fit a separate intercept term for linear regression models, so we don't include the all-ones entry in our features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_summer[\"High_temp\"].values.reshape((-1,1)) # if we pass a 1-feature array we need to re-shape it! This is not required for multi-dimenisonal arrays\n",
    "y = df_summer[\"MAX\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can import and initialize our model. In general, scikit-learn has a different class for each different type of learning algorithm. In this case, we are importing the LinearRegression class. When we initialize the class, we pass various parameters to the constructor. In this case, we are specifying that we will fit an intercept term (i.e., we will not pass it as an explicit feature), and that we will not normalize the data. These are in fact the default parameters of the linear regression class, but we include them explicitly just for illustration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `scikit-learn` also has a pre-implemented scaling functionality such as the `StandardScaler` class ([see here](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)). For convenience we will not scale the data here. Gradient descent is still able to find an optimal solution. For higher dimensional inputs, however, scaling is highly recommended. We will cover this in future workshops!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize model\n",
    "linear_model = LinearRegression(fit_intercept=True, normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that when we create this class we haven't actually passed any data to the system.  This is the standard interface for scikit-learn classes: the constructor just initializes the hyperparameters of the model, and when we actually want to fit it to data, we call the `model.fit()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model\n",
    "linear_model.fit(X, y)\n",
    "print(linear_model.coef_, linear_model.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare to own implementation (at 1000 iteration of gradient descent)\n",
    "theta_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, when we want to make a prediction on a new data point, we call the model.predict() function, passing in the feature values for the new points we want to predict. In the following example, we would be predicting what the peak demand would be given a 25 degree peak temperature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Predict the peak load at 40 deg c using our defined theta_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict on new data\n",
    "Xnew = np.array([[40]]) #predict peak load at 25 deg c\n",
    "\n",
    "print(linear_model.predict(Xnew))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we can pass multiple points to predict at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xnew = np.array([[25],[23]])\n",
    "print(linear_model.predict(Xnew))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Scikit-Learn Library for fitting linear regression line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(xp.values.reshape(-1,1), yp)\n",
    "model_pred = lr.predict(xp.values.reshape(-1,1))\n",
    "\n",
    "\n",
    "fig,ax = plt.subplots(figsize = (8,6))\n",
    "\n",
    "ax.scatter(xp, yp, marker=\"x\")\n",
    "ax.set_xlabel(\"High Temperature (°C)\")\n",
    "ax.set_ylabel(\"Peak Demand (GW)\")\n",
    "\n",
    "ax.plot(xp, model_pred, c='C2')\n",
    "ax.legend(['Squared loss fit','Observed days'])\n",
    "#plt.savefig('summer data_peak demand_line.pdf')\n",
    "print(lr.coef_, lr.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Regression Model Evaluation`\n",
    "\n",
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = df_summer[\"MAX\"]\n",
    "y_pred = linear_model.predict(df_summer[\"High_temp\"].values.reshape((-1,1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (Root) Mean Squared Error ((R)MSE)\n",
    "The MSE calculates the mean of the squared residuals. As the fitting of the linear regression model is (typically) carried out by minimizing the squared error (see loss function in lecture slides), the MSE corresponds directly to the model optimization. The unit of the MSE is the target unit squared, i.e. in the above example using energy consumption in GW, the unit of the MSE is $(GW)^2$. As the residuals (the difference of prediction and observed values) are squared, large deviations are penalized stronger than small deviations. Therefore, the weight of outliers increases using MSE as a metric. This is useful in applications, where small prediction errors are not important, but large errors have to be avoided.\n",
    "\n",
    "As the squared unit is hard to interpret, the RMSE can be used instead of the MSE. The RMSE is just the square root of the MSE, meaning that it is monotonic with respect to the MSE - a bigger MSE leads to a bigger RMSE and vice versa. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Mean Squared Error:\",mean_squared_error(y_pred, y_true),\"(GW)^2\")\n",
    "print(\"Root Mean Squared Error:\",mean_squared_error(y_pred, y_true)**0.5,\"GW\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RMSE can be easily interpreted as it can be set into relation with the c. 1.8 GW average demand mean. Generally, MSE and RMSE have always to be assessed given the model under evaluation. A MSE or RMSE with a certain value has no meaning unless the order of magnitude of the dataset is known (with an exception of the very unlikely case that the MSE is zero). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mean Absolute Error (MAE)\n",
    "Just as the RSME, the MAE is easy to interpret. It is just the mean absolute value of the error term. When there are no or little major outliers, RMSE and MAE often have the same order of magnitude, with the MAE always being smaller than the RMSE. While a linear regression is typically fitted using the least squares method, the MAE can still be a valuable metric. It is applicable when large errors are not disproportionately worse than smaller errors. For example, when prediciting monetary values, an error of 100 USD might always be twice as bad as an error of 50 USD and so on. If this is the case, the MAE can actually be the more suitable error metric. As the RSME (and the MAE for that matter), the MAE does only make sense when knowing the order of magnitude of the predicted values. On its own, it does not imply good or bad model fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Mean Absolute Error:\",mean_absolute_error(y_pred, y_true),\"GW\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Coefficient of Determination ($R^2$)\n",
    "The coefficient of determination (typically pronunced as \"R squared\"), is a metric of how well a model explains variance seen in the data. $R^2$ indicates the ratio of the explained and the overall variance (given some assumptions that we will not discuss here). Its value is always between 0 and 1. It can therefore be used as a means of comparison not only between regression methods, but also between completely different datasets - or even without knowing the data at all. For example, a value of 0.9 is always good, a value of 0.1 is always bad. Still, what threshold you would define as a good model fit depends on the domain of application. If you except a high degree of randomness in your data, it is harder to explain your variance using a predictor. If you expect your data to be highly deterministic, then it should be easily explainable using suitable features and prediction methods. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Coefficient of determination:\",r2_score(y_pred, y_true))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Other metrics\n",
    "Scikit-learn does include other metrics that are not used as often as the ones explained above. To read up on those, just visit Scikit-learn's website. You can also define functions to calculate your own error metrics - depending on the application and with sound reasoning. E.g., it could make sense to weigh negative deviations higher than positive deviations, or the relative error instead of a form of absolute error could be decisive. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "[Model Evaluation](http://scikit-learn.org/stable/modules/model_evaluation.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Note that the metrcis calculated above represent the training loss! They do not say anything about predictive performance!__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of Class Exercise\n",
    "\n",
    "**Exercise 1**:\n",
    "\n",
    "- Fit a linear regression by using `scikit-learn` library for average temperature and average demand data.\n",
    "- Plot the data and fitted line. Annotate your graph and size it appropriately.\n",
    "- Present relevant test metrics to quantify the __training__ loss \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "# define target and feature vectors\n",
    "\n",
    "\n",
    "\n",
    "# initialize lin model\n",
    "\n",
    "\n",
    "\n",
    "# fit model\n",
    "\n",
    "\n",
    "\n",
    "# predict training data\n",
    "\n",
    "\n",
    "\n",
    "# define y true\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Present test_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2**:\n",
    "\n",
    "(we will cover this in detail in next weeks tutorial)\n",
    "\n",
    "\n",
    "- Create an `IsWeekday` feature and examine its effect on peak demand and high temperature data.\n",
    "- Using `Scikit learn` build a linear model that incorporates `IsWeekday` as an additional feature.\n",
    "- Plot the data and fitted line. Annotate your graph and size it appropriately.\n",
    "- Provide appropriate test metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "\n",
    "\n",
    "# see WS08 for solution\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a0e5296c3657c6b7a86a9bab3436e28ffbdd8356439efa15fab08846068601a4"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
